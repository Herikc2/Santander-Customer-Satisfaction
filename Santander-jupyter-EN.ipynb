{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A predictive model will be created to predict whether a customer is satisfied or dissatisfied. Historical data provided by Santandar will be used.\n",
    "\n",
    "Dataset: https://www.kaggle.com/c/santander-customer-satisfaction/overview\n",
    "\n",
    "The dataset has anonymous data from more than 70 thousand Santander customers, separated by the bank itself into two datasets, the first for training and the second for testing.\n",
    "\n",
    "The \"TARGET\" column is the variable to be predicted. It is equal to one for dissatisfied customers and 0 for satisfied customers.\n",
    "\n",
    "The task is to predict the likelihood that each customer in the test suite is a dissatisfied customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from libraries\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting and Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files were uploaded in CSV format, Santander provided one file for training and another for testing. However, during the project it was decided to use the training base for training and tests, since the test base does not have a \"TARGET\" column for future performance checks on the model. At the end of the data processing, the base is separated from the sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the training dataset in CSV format\n",
    "training_file = 'data/train.csv'\n",
    "test_file = 'data/test.csv'\n",
    "data_training = pd.read_csv(training_file)\n",
    "test_data = pd.read_csv (test_file)\n",
    "print(data_training.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the first 20 lines\n",
    "data_training.head (20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type of each attribute\n",
    "data_training.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Summary\n",
    "data_training.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of classes\n",
    "data_training.groupby(\"TARGET\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, there are many more satisfied customers (class 0) than dissatisfied customers (class 1), so the dataset is totally unbalanced. Thus, it is chosen to perform a simple balancing, based on the division of data by class and capturing a sample of class 0 that has more data, this sample is the same size as class 1. Thus, you will have a fully balanced dataset with aa Class 0 and 1 in the same quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing by class\n",
    "data_class_0 = data_training[data_training['TARGET'] == 0]\n",
    "data_class_1 = data_training[data_training['TARGET'] == 1]\n",
    "\n",
    "counter_class_0 = data_class_0.shape[0]\n",
    "contador_classe_1 = data_class_1.shape[0]\n",
    "\n",
    "data_class_0_sample = data_class_0.sample(counter_class_0)\n",
    "training_data = pd.concat([data_class_0_sample, data_class_1], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, Pearson's correlation is used to identify the attributes that have minimal correlation above the limit. In this way it is possible to guarantee the variables with the best performance. As it is a dataset with many columns (371), no variable has a prominent correlation, so I chose to put a significant minimum value to reduce the variables by at least half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation\n",
    "data_training.corr(method = 'pearson')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the correlation between the target variable and the predictor variables\n",
    "corr = training_data[training_data.columns [1:]].corr()['TARGET'][:].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_correlation = 0.02\n",
    "corr2 = corr[corr > minimal_correlation]\n",
    "corr2.shape\n",
    "corr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_keys = corr2.index.tolist()\n",
    "data_filter = data_training[corr_keys]\n",
    "data_filter.head(20)\n",
    "data_filter.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the columns are filtered according to the Pearson correlation and the normalization of the predictive data is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering only the columns that have a correlation above the minimum variable\n",
    "array_treino = data_training[corr_keys].values\n",
    "\n",
    "# Separating the array into input and output components for training data\n",
    "X = array_treino[:, 0:array_treino.shape[1] - 1]\n",
    "Y = array_treino[:, array_treino.shape[1] - 1]\n",
    "\n",
    "# Creating the training and test dataset\n",
    "test_size = 0.30\n",
    "X_training, X_testing, Y_training, Y_testing = train_test_split(X, Y, test_size = test_size)\n",
    "\n",
    "# Generating normalized data\n",
    "scaler = Normalizer (). fit (X_training)\n",
    "normalizedX_treino = scaler.transform(X_training)\n",
    "\n",
    "scaler = Normalizer().fit(X_testing)\n",
    "normalizedX_teste = scaler.transform(X_testing)\n",
    "Y_training = Y_training.astype('int')\n",
    "Y_testing = Y_testing.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution of a series of classification algorithms is based on those that have the best result. For this test, the training base is used without any treatment or data selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the number of folds for cross validation\n",
    "num_folds = 10\n",
    "\n",
    "# Preparing the list of models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('SVM', SVC()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "names = []\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = KFold (n_splits = num_folds)\n",
    "    cv_results = cross_val_score (model, X_training, Y_training, cv = kfold, scoring = 'accuracy')\n",
    "    results.append (cv_results)\n",
    "    names.append (name)\n",
    "    msg = \"% s:% f (% f)\"% (name, cv_results.mean (), cv_results.std ())\n",
    "    print (msg)\n",
    "\n",
    "# Boxplot to compare the algorithms\n",
    "fig = plt.figure ()\n",
    "fig.suptitle ('Comparison of Classification Algorithms')\n",
    "ax = fig.add_subplot (111)\n",
    "plt.boxplot (results)\n",
    "ax.set_xticklabels (names)\n",
    "plt.show ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some tests, the final training is started with the chosen algorithms, based on their respective performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the performance of the model and save it in a pickle format for future reuse.\n",
    "def model_report(model_name):\n",
    "    # Print result\n",
    "    print(\"Accuracy:% .3f\"% score)\n",
    "    \n",
    "    # Making predictions and building the Confusion Matrix\n",
    "    predictions = result.predict(X_testing)\n",
    "    matrix = confusion_matrix(Y_testing, predictions)\n",
    "    print(matrix)\n",
    "    \n",
    "    report = classification_report(Y_testing, predictions)\n",
    "    print(report)\n",
    "    \n",
    "    # The precision matrix is ​​created to visualize the number of correct cases\n",
    "    labels = ['SATISFIED', 'UNSATISFIED']\n",
    "    cm = confusion_matrix(Y_testing, predictions)\n",
    "    cm = pd.DataFrame(cm, index = ['0', '1'], columns = ['0', '1'])\n",
    "     \n",
    "    plt.figure(figsize = (10.10))\n",
    "    sns.heatmap(cm, cmap = \"Blues\", linecolor = 'black', linewidth = 1, annot = True, fmt = '', xticklabels = labels, yticklabels = labels)\n",
    "    \n",
    "    # Saving the model\n",
    "    file = 'models/final_classifier_model' + model_name + '.sav'\n",
    "    pickle.dump (model, open(file, 'wb'))\n",
    "    print(\"Saved Model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "model = LogisticRegression()\n",
    "result = model.fit(normalizedX_treino, Y_testing)\n",
    "score = result.score(normalizedX_treino, Y_testing)\n",
    "model_report(\"LR\")\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "model = LinearDiscriminantAnalysis()\n",
    "result = model.fit(X_training, Y_testing)\n",
    "score = result.score(X_training, Y_testing)\n",
    "model_report(\"LDA\")\n",
    "\n",
    "# KNN\n",
    "model = KNeighborsClassifier()\n",
    "result = model.fit(normalizedX_treino, Y_testing)\n",
    "score = result.score(normalizedX_treino, Y_testing)\n",
    "model_report(\"KNN\")\n",
    "\n",
    "# CART\n",
    "model = DecisionTreeClassifier()\n",
    "result = model.fit(X_training, Y_testing)\n",
    "score = result.score(X_training, Y_testing)\n",
    "model_report(\"CART\")\n",
    "\n",
    "# XGBOOST\n",
    "model = XGBClassifier()\n",
    "result = model.fit(X_training, Y_testing)\n",
    "score = result.score(X_training, Y_testing)\n",
    "model_report(\"XGBOOST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "file = 'models model_classifier_final_XGBOOST.sav'\n",
    "model_classifier = pickle.load(open(file, 'rb'))\n",
    "model_prod = model_classifier.score(X_testing, Y_testing)\n",
    "print(\"Uploaded Model\")\n",
    "\n",
    "# Print Result\n",
    "print(\"Accuracy:% .3f\"% (model_prod.mean () * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing several tests it was seen that the models with the best accuracy were LDA, KNN AND XGBOOST. These models showed accuracy greater than 70%. Even so, the XGBOOST is more accurate with 75%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
